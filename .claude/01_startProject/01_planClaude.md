# Document de Conception MaÃ®tre
## SystÃ¨me Autonome de Benchmarking Ã‰volutif (SABE)
### "La Sentinelle des LLM"

---

**Version:** 1.0
**Date:** 24 novembre 2025
**Statut:** Document Fondateur - Bible du Projet
**Auteur:** Tom @ Groupe Conseil ERA

---

## Table des MatiÃ¨res

1. [Vision et Philosophie](#1-vision-et-philosophie)
2. [Glossaire et Concepts Fondamentaux](#2-glossaire-et-concepts-fondamentaux)
3. [PÃ©rimÃ¨tre et Contraintes](#3-pÃ©rimÃ¨tre-et-contraintes)
4. [Architecture Conceptuelle](#4-architecture-conceptuelle)
5. [Domaine et EntitÃ©s](#5-domaine-et-entitÃ©s)
6. [SpÃ©cifications Fonctionnelles par Module](#6-spÃ©cifications-fonctionnelles-par-module)
7. [Flux et Comportements du SystÃ¨me](#7-flux-et-comportements-du-systÃ¨me)
8. [Dimensions Temporelles et MÃ©moire](#8-dimensions-temporelles-et-mÃ©moire)
9. [ModÃ¨le de DonnÃ©es Conceptuel](#9-modÃ¨le-de-donnÃ©es-conceptuel)
10. [Priorisation MoSCoW](#10-priorisation-moscow)
11. [Roadmap par Phases](#11-roadmap-par-phases)
12. [ConsidÃ©rations Transversales](#12-considÃ©rations-transversales)
13. [Questions Ouvertes et DÃ©cisions Ã  Prendre](#13-questions-ouvertes-et-dÃ©cisions-Ã -prendre)
14. [Annexes](#14-annexes)

---

## 1. Vision et Philosophie

### 1.1 Ã‰noncÃ© de Vision

> Construire un **observatoire autonome et perpÃ©tuel de l'intelligence artificielle** : un systÃ¨me qui agit comme un auditeur systÃ©matique des LLM, mesurant non seulement si un modÃ¨le fonctionne, mais sa "texture", son Ã©volution dans le temps, et sa rÃ©action face Ã  des contextes changeants. Le systÃ¨me accumule de la donnÃ©e historique pour visualiser des tendances et permettre des dÃ©cisions Ã©clairÃ©es sur le choix des modÃ¨les.

### 1.2 Objectifs StratÃ©giques

| Objectif | Description |
|----------|-------------|
| **Comparaison systÃ©matique** | Ã‰valuer automatiquement et rÃ©guliÃ¨rement les performances de multiples modÃ¨les sur des jeux de questions dÃ©finis |
| **Ã‰volution temporelle** | Mesurer comment un mÃªme modÃ¨le Ã©volue dans le temps (amÃ©lioration, dÃ©gradation, mise Ã  jour silencieuse) |
| **MÃ©moire structurÃ©e** | Garder trace de tous les runs, rankings, itÃ©rations et conditions de test |
| **Autonomie complÃ¨te** | Fonctionner sans intervention humaine une fois configurÃ© |
| **Fondation dÃ©cisionnelle** | Servir de base pour choisir automatiquement le bon modÃ¨le par use case dans des produits futurs (AgentHub, etc.) |

### 1.3 Principes Fondamentaux

#### Autonomie
Le systÃ¨me fonctionne comme une entitÃ© indÃ©pendante. Une fois configurÃ©, il exÃ©cute ses benchmarks, calcule ses rankings, et met Ã  jour son historique sans nÃ©cessiter d'intervention humaine.

#### DÃ©terminisme du Pipeline
Bien que les LLM ne soient pas dÃ©terministes par nature, le pipeline du systÃ¨me l'est. MÃªme sÃ©quence d'Ã©tapes, mÃªme rÃ©action aux mÃªmes entrÃ©es, mÃªme comportement reproductible.

#### SÃ©paration des PrÃ©occupations
Chaque concept du systÃ¨me est isolÃ© et indÃ©pendant :
- La connexion aux providers = son propre module
- Le systÃ¨me de ranking = son propre module
- La gestion temporelle = son propre module
- Chaque type de question = son propre concept

#### Approche IncrÃ©mentale
Le systÃ¨me est pensÃ© et construit de maniÃ¨re step-by-step :
- Chaque phase clairement dÃ©limitÃ©e
- Chaque Ã©tape indÃ©pendamment testable
- Chaque feature isolable
- Chaque bout de code encapsulÃ©

#### ReproductibilitÃ©
Toute exÃ©cution de benchmark doit Ãªtre :
- **TraÃ§able** : quand, avec quels paramÃ¨tres, dans quel contexte
- **Reproductible** : mÃªme entrÃ©e = mÃªme comportement systÃ¨me
- **Comparable** : entre diffÃ©rentes dates, modÃ¨les, versions

### 1.4 CaractÃ©ristiques ClÃ©s du SystÃ¨me

| CaractÃ©ristique | Manifestation |
|-----------------|---------------|
| **SimplicitÃ© d'interaction** | Peu d'interactions requises malgrÃ© la complexitÃ© interne |
| **Profondeur architecturale** | Architecture profonde avec de nombreux concepts imbriquÃ©s |
| **ExtensibilitÃ© native** | FacilitÃ© d'ajout de nouveaux modÃ¨les, providers, types de tests |
| **Persistance totale** | Aucune donnÃ©e n'est perdue, tout est historisÃ© |

---

## 2. Glossaire et Concepts Fondamentaux

### 2.1 Concepts LiÃ©s aux Providers

| Terme | DÃ©finition |
|-------|------------|
| **Provider** | Service externe fournissant l'accÃ¨s Ã  des modÃ¨les LLM via API. V1 : OpenRouter |
| **Provider Gateway** | Interface d'abstraction permettant l'agnosticisme du fournisseur |
| **Connexion API** | MÃ©canisme technique de communication avec un provider (credentials, endpoints, retry policy) |

### 2.2 Concepts LiÃ©s aux ModÃ¨les

| Terme | DÃ©finition |
|-------|------------|
| **ModÃ¨le/LLM** | Un Large Language Model spÃ©cifique accessible via un provider (ex: `gpt-4-0613`, `claude-3.5-sonnet`) |
| **MÃ©tadonnÃ©es Statiques** | Informations fixes : fournisseur, date de sortie, taille, coÃ»t par token |
| **MÃ©tadonnÃ©es Dynamiques** | Tags et labels attribuÃ©s par le systÃ¨me (ex: "Coding Expert", "Fast Inference", "Deprecated") |
| **Configuration de ModÃ¨le** | ParamÃ¨tres d'appel : temperature, max_tokens, top_p, etc. |

### 2.3 Concepts LiÃ©s aux Tests

| Terme | DÃ©finition |
|-------|------------|
| **Question de Benchmark** | Un prompt standardisÃ© utilisÃ© pour Ã©valuer les modÃ¨les |
| **Type de Question** | CatÃ©gorisation par nature : raisonnement, code, crÃ©ativitÃ©, factualitÃ©, etc. |
| **Suite de Questions** | Ensemble cohÃ©rent de questions formant un benchmark complet |
| **Campagne de Benchmark** | Configuration d'exÃ©cution : quelles suites, quels modÃ¨les, quelle frÃ©quence |

### 2.4 Concepts LiÃ©s aux Rankings

| Terme | DÃ©finition |
|-------|------------|
| **Ranking** | Classement ordonnÃ© de modÃ¨les selon des critÃ¨res spÃ©cifiques. **EntitÃ© complexe avec sa propre table** |
| **Score** | Valeur numÃ©rique reprÃ©sentant la performance sur un critÃ¨re |
| **Dimension de Ranking** | Axe d'Ã©valuation : par type de question, par date, par itÃ©ration, comparatif |
| **Position** | Place d'un modÃ¨le dans un classement donnÃ©, avec delta vs prÃ©cÃ©dent |

### 2.5 Concepts Temporels

| Terme | DÃ©finition |
|-------|------------|
| **Run/ExÃ©cution** | Une instance d'exÃ©cution complÃ¨te du systÃ¨me de benchmark |
| **ItÃ©ration** | Version globale d'une expÃ©rience (changement de dataset, prompt, mÃ©thode d'Ã©valuation) |
| **Snapshot** | Capture d'Ã©tat du systÃ¨me Ã  un instant T : rankings, scores, configuration |
| **PÃ©riode de Comparaison** | FenÃªtre temporelle : WoW (Week-over-Week), MoM (Month-over-Month), YoY (Year-over-Year) |

### 2.6 Concepts d'Ã‰valuation

| Terme | DÃ©finition |
|-------|------------|
| **LLM-as-Judge** | Utilisation d'un LLM puissant pour noter les rÃ©ponses d'autres LLM |
| **Ã‰valuation Automatique** | Ã‰valuation programmatique : exact match, regex, tests unitaires, mÃ©triques NLP |
| **RÃ©sultat d'Ã‰valuation** | Score obtenu + justification + mÃ©thode utilisÃ©e |

### 2.7 Concepts de MÃ©moire

| Terme | DÃ©finition |
|-------|------------|
| **Historique** | Archive complÃ¨te de tous les rÃ©sultats passÃ©s |
| **MÃ©moire du SystÃ¨me** | Ã‰tat persistant permettant la continuitÃ© entre exÃ©cutions |
| **Context Snapshot** | Ã‰tat du monde au moment du test (actualitÃ©s, Ã©vÃ©nements) pour analyse contextuelle |

---

## 3. PÃ©rimÃ¨tre et Contraintes

### 3.1 PÃ©rimÃ¨tre V1 (In-Scope)

| Domaine | Inclus |
|---------|--------|
| **Providers** | IntÃ©gration API avec OpenRouter uniquement |
| **ModÃ¨les** | Gestion d'un catalogue de modÃ¨les Ã  benchmarker avec labels personnalisÃ©s |
| **Benchmarks** | DÃ©finition et stockage de suites de benchmark (tÃ¢ches, prompts, consignes) |
| **Orchestration** | Runs planifiÃ©s automatiques (hebdomadaires, mensuels) |
| **Stockage** | Toutes les rÃ©ponses brutes + mÃ©tadonnÃ©es complÃ¨tes |
| **Ã‰valuation** | RÃ¨gles simples + LLM-as-Judge |
| **Rankings** | Multi-dimensions avec snapshots horodatÃ©s |
| **TemporalitÃ©** | Comparaisons semaine/semaine, mois/mois |
| **MÃ©moire** | ItÃ©rations, config versioning, historique complet |

### 3.2 Hors PÃ©rimÃ¨tre V1 (Out-of-Scope)

| Domaine | Exclu pour V1 |
|---------|---------------|
| **UI avancÃ©e** | Interface graphique Ã©laborÃ©e d'Ã©dition des benchmarks |
| **Auto-sÃ©lection** | Choix automatique de modÃ¨les en prod basÃ© sur les rÃ©sultats |
| **Optimisation coÃ»ts** | Choix modÃ¨le par ratio coÃ»t/performance automatique |
| **Multi-mÃ©dia** | Benchmarks image, audio, vidÃ©o |
| **Multi-providers** | OpenAI direct, Anthropic direct, LLM locaux |

### 3.3 Contraintes Techniques

| Contrainte | Description |
|------------|-------------|
| **Provider unique** | OpenRouter en V1 (mais architecture extensible) |
| **Lecture prioritaire** | V1 orientÃ©e consultation des rÃ©sultats, pas Ã©dition complexe |
| **CoÃ»ts API** | Budget Ã  dÃ©finir pour les appels LLM (tests + juges) |
| **Latence acceptable** | Les benchmarks peuvent tourner sur plusieurs heures si nÃ©cessaire |

### 3.4 Contraintes Business

| Contrainte | Description |
|------------|-------------|
| **Autonomie** | Minimum d'interventions humaines au quotidien |
| **AuditabilitÃ©** | TraÃ§abilitÃ© complÃ¨te des dÃ©cisions et rÃ©sultats |
| **ConfidentialitÃ©** | Flag pour indiquer si un benchmark utilise des donnÃ©es sensibles |

---

## 4. Architecture Conceptuelle

### 4.1 Vue en Couches

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       COUCHE PRÃ‰SENTATION                               â”‚
â”‚         Rapports, Visualisations, Exports, API de consultation          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       COUCHE ANALYTIQUE                                 â”‚
â”‚           Rankings, Comparaisons, AgrÃ©gations, Tendances                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       COUCHE Ã‰VALUATION                                 â”‚
â”‚         LLM-as-Judge, Ã‰valuation Auto, Scoring, Normalisation           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       COUCHE EXÃ‰CUTION                                  â”‚
â”‚           Orchestration, SÃ©quencement, Gestion des itÃ©rations           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       COUCHE INTÃ‰GRATION                                â”‚
â”‚        Abstraction Providers, Connexions API, Gestion credentials       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       COUCHE PERSISTANCE                                â”‚
â”‚            Base de donnÃ©es, Historique, Snapshots, Cache                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Modules Fonctionnels

| Module | ResponsabilitÃ© | EntrÃ©es | Sorties |
|--------|----------------|---------|---------|
| **Provider Manager** | Gestion des connexions aux providers | Config provider, credentials | Connexion active, rÃ©ponses normalisÃ©es |
| **Model Registry** | Catalogue des modÃ¨les disponibles | DÃ©finitions, mÃ©tadonnÃ©es | Liste des modÃ¨les actifs, configurations |
| **Question Bank** | Gestion des questions de benchmark | Questions, catÃ©gorisations | Suites de questions, prompts formatÃ©s |
| **Benchmark Executor** | Orchestration des exÃ©cutions | Profil, modÃ¨les, questions | RÃ©ponses brutes, mÃ©tadonnÃ©es d'exÃ©cution |
| **Evaluator** | Ã‰valuation des rÃ©ponses | RÃ©ponses, critÃ¨res, modÃ¨le juge | Scores, justifications |
| **Ranking Engine** | Calcul et gestion des rankings | Scores, critÃ¨res de ranking | Classements multi-dimensionnels |
| **Temporal Analyzer** | Analyses temporelles et comparaisons | Historique, pÃ©riodes | Tendances, Ã©volutions, deltas |
| **Reporter** | GÃ©nÃ©ration des rapports | DonnÃ©es analysÃ©es, format | Rapports formatÃ©s |
| **Scheduler** | Planification des exÃ©cutions | Configuration planning | DÃ©clenchement des benchmarks |
| **Memory Manager** | Gestion de la persistance | DonnÃ©es Ã  persister | DonnÃ©es historiques, Ã©tat systÃ¨me |

### 4.3 Interactions entre Modules

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Scheduler   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ dÃ©clenche
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Model Registryâ”‚â—„â”€â”€â”€â”‚  Benchmark   â”‚â”€â”€â”€â–ºâ”‚Question Bank â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Executor   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ appelle
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Provider   â”‚
                    â”‚   Manager    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ rÃ©ponses
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Evaluator   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ scores
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Ranking    â”‚â”€â”€â”€â–ºâ”‚   Temporal   â”‚
                    â”‚    Engine    â”‚    â”‚   Analyzer   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼            â–¼            â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Memory   â”‚ â”‚ Reporter â”‚ â”‚ Database â”‚
       â”‚ Manager  â”‚ â”‚          â”‚ â”‚          â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Domaine et EntitÃ©s

### 5.1 Provider Gateway

**RÃ´le :** Interface d'abstraction pour l'agnosticisme du fournisseur.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| name | String | Nom du provider (OpenRouter, OpenAI, etc.) |
| api_base_url | String | URL de base de l'API |
| auth_type | Enum | Type d'authentification (API Key, OAuth, etc.) |
| status | Enum | actif / inactif |
| rate_limits | JSON | Limites de requÃªtes |
| config | JSON | Configuration spÃ©cifique |
| created_at | DateTime | Date d'ajout |

### 5.2 ModÃ¨le LLM

**RÃ´le :** ReprÃ©sentation d'un modÃ¨le de langage. C'est un objet vivant qui Ã©volue.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique interne |
| provider_id | FK | RÃ©fÃ©rence au provider |
| provider_model_id | String | ID technique chez le provider (ex: `anthropic/claude-3-opus`) |
| display_name | String | Nom d'affichage |
| label | String | Label personnalisÃ© dÃ©fini par l'utilisateur |
| description | Text | Description courte |
| status | Enum | actif / inactif / expÃ©rimental / deprecated |
| context_size | Integer | Taille du contexte en tokens |
| cost_input_per_token | Decimal | CoÃ»t par token en entrÃ©e |
| cost_output_per_token | Decimal | CoÃ»t par token en sortie |
| release_date | Date | Date de sortie officielle |
| tags | Array[String] | Ã‰tiquettes (reasoning, code, fast, cheap, etc.) |
| default_config | JSON | ParamÃ¨tres par dÃ©faut (temperature, etc.) |
| metadata | JSON | Autres mÃ©tadonnÃ©es |
| created_at | DateTime | Date d'ajout au systÃ¨me |

### 5.3 Question de Benchmark

**RÃ´le :** Un stimulus standardisÃ© envoyÃ© aux modÃ¨les.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| content | Text | Contenu du prompt |
| question_type_id | FK | RÃ©fÃ©rence au type de question |
| difficulty | Enum | Estimation de difficultÃ© (easy/medium/hard/expert) |
| expected_answer | Text | RÃ©ponse attendue (si applicable) |
| evaluation_criteria | JSON | CritÃ¨res d'Ã©valuation spÃ©cifiques |
| evaluation_method | Enum | exact_match / regex / llm_judge / custom |
| weight | Decimal | Poids dans le score global |
| version | Integer | NumÃ©ro de version |
| status | Enum | active / archived |
| language | String | Langue de la question |
| context | Text | Contexte additionnel si nÃ©cessaire |
| created_at | DateTime | Date de crÃ©ation |

### 5.4 Type de Question

**RÃ´le :** CatÃ©gorisation des questions par nature.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| name | String | Nom (Reasoning, Code, Creativity, etc.) |
| description | Text | Description |
| weight | Decimal | PondÃ©ration dans le score global |
| evaluation_defaults | JSON | MÃ©thodes d'Ã©valuation par dÃ©faut |
| created_at | DateTime | Date de crÃ©ation |

**Types PrÃ©vus :**
- Raisonnement logique
- MathÃ©matiques
- CrÃ©ativitÃ©
- FactualitÃ©
- Suivi d'instructions
- Coding
- Analyse de texte
- Multi-Ã©tapes
- Traduction
- RÃ©sumÃ©

### 5.5 Suite de Questions

**RÃ´le :** Ensemble cohÃ©rent de questions formant un benchmark complet.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| name | String | Nom de la suite |
| description | Text | Description |
| type | String | Type global (generaliste, code, reasoning) |
| version | String | Version (v1, v2, etc.) |
| status | Enum | active / deprecated |
| objective | Text | Objectif de la suite |
| created_at | DateTime | Date de crÃ©ation |

### 5.6 Campagne de Benchmark

**RÃ´le :** Configuration d'une sÃ©rie de benchmarks planifiÃ©s.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| name | String | Nom (Weekly Global, Monthly Deep, etc.) |
| description | Text | Description |
| frequency | Enum | weekly / monthly / quarterly / manual |
| cron_expression | String | Expression cron pour planification |
| suite_ids | Array[UUID] | Suites incluses |
| model_selection_rule | JSON | RÃ¨gle de sÃ©lection des modÃ¨les |
| is_active | Boolean | Campagne active ou non |
| last_run_at | DateTime | DerniÃ¨re exÃ©cution |
| next_run_at | DateTime | Prochaine exÃ©cution |
| created_at | DateTime | Date de crÃ©ation |

### 5.7 ExÃ©cution de Benchmark (Run)

**RÃ´le :** Une instance d'exÃ©cution complÃ¨te du systÃ¨me.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| campaign_id | FK | RÃ©fÃ©rence Ã  la campagne |
| iteration_id | FK | RÃ©fÃ©rence Ã  l'itÃ©ration |
| iteration_number | Integer | NumÃ©ro sÃ©quentiel du run |
| started_at | DateTime | Date/heure de dÃ©but |
| completed_at | DateTime | Date/heure de fin |
| status | Enum | pending / running / completed / failed / partial |
| models_count | Integer | Nombre de modÃ¨les testÃ©s |
| questions_count | Integer | Nombre de questions posÃ©es |
| config_snapshot | JSON | Snapshot de la configuration au moment du run |
| summary | JSON | RÃ©sumÃ© des rÃ©sultats agrÃ©gÃ©s |
| error_log | Text | Log des erreurs si applicable |
| created_at | DateTime | Date de crÃ©ation |

### 5.8 RÃ©sultat de Benchmark

**RÃ´le :** RÃ©ponse brute d'un modÃ¨le Ã  une question.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| execution_id | FK | RÃ©fÃ©rence au run |
| model_id | FK | RÃ©fÃ©rence au modÃ¨le |
| question_id | FK | RÃ©fÃ©rence Ã  la question |
| input_effective | Text | Prompt final envoyÃ© |
| response_content | Text | RÃ©ponse brute du modÃ¨le |
| response_time_ms | Integer | Temps de rÃ©ponse en millisecondes |
| tokens_input | Integer | Tokens en entrÃ©e |
| tokens_output | Integer | Tokens en sortie |
| cost | Decimal | CoÃ»t de l'appel |
| status | Enum | success / error / timeout |
| error_message | Text | Message d'erreur si applicable |
| raw_response | JSON | RÃ©ponse brute complÃ¨te de l'API |
| created_at | DateTime | Date de crÃ©ation |

### 5.9 Ã‰valuation

**RÃ´le :** Notation d'une rÃ©ponse selon des critÃ¨res dÃ©finis.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| result_id | FK | RÃ©fÃ©rence au rÃ©sultat de benchmark |
| evaluator_type | Enum | rule_based / llm_judge / custom |
| evaluator_model_id | FK | ModÃ¨le juge (si LLM-as-Judge) |
| dimension | String | Dimension Ã©valuÃ©e (accuracy, style, etc.) |
| score | Decimal | Score obtenu |
| max_score | Decimal | Score maximum possible |
| normalized_score | Decimal | Score normalisÃ© (0-100) |
| justification | Text | Justification de la note |
| criteria_scores | JSON | Scores dÃ©taillÃ©s par critÃ¨re |
| evaluation_prompt | Text | Prompt utilisÃ© pour l'Ã©valuation (si LLM) |
| created_at | DateTime | Date de crÃ©ation |

### 5.10 Ranking (Table Complexe DÃ©diÃ©e)

**RÃ´le :** Classement des modÃ¨les. C'est une entitÃ© relationnelle complexe avec sa propre table.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| execution_id | FK | RÃ©fÃ©rence au run |
| ranking_type | Enum | global / by_question_type / by_suite / comparative |
| dimension | String | Ce sur quoi on classe (overall, accuracy, speed, etc.) |
| scope | JSON | Filtres appliquÃ©s (pÃ©riode, modÃ¨les comparÃ©s, etc.) |
| metadata | JSON | MÃ©tadonnÃ©es additionnelles |
| created_at | DateTime | Date de crÃ©ation |

### 5.11 EntrÃ©e de Ranking

**RÃ´le :** Position d'un modÃ¨le dans un ranking donnÃ©.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| ranking_id | FK | RÃ©fÃ©rence au ranking |
| model_id | FK | RÃ©fÃ©rence au modÃ¨le |
| position | Integer | Position dans le classement (1er, 2e, etc.) |
| score | Decimal | Score associÃ© |
| previous_position | Integer | Position prÃ©cÃ©dente (nullable) |
| delta_position | Integer | Changement de position |
| delta_score | Decimal | Changement de score |
| sample_size | Integer | Nombre de questions Ã©valuÃ©es |
| metadata | JSON | DÃ©tails additionnels |

### 5.12 ItÃ©ration

**RÃ´le :** Version globale d'une expÃ©rience de benchmark.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| code | String | Code unique (ITER-2025-01) |
| name | String | Nom descriptif |
| description | Text | Description des changements |
| changelog | Text | Liste dÃ©taillÃ©e des modifications |
| started_at | DateTime | Date de dÃ©but |
| ended_at | DateTime | Date de fin (nullable si active) |
| is_current | Boolean | ItÃ©ration active |
| config_snapshot | JSON | Configuration complÃ¨te de l'itÃ©ration |
| created_at | DateTime | Date de crÃ©ation |

### 5.13 Snapshot

**RÃ´le :** Capture d'Ã©tat du systÃ¨me Ã  un instant T.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| execution_id | FK | RÃ©fÃ©rence au run |
| snapshot_type | Enum | ranking / config / full |
| data | JSON | DonnÃ©es capturÃ©es |
| created_at | DateTime | Date de crÃ©ation |

### 5.14 Comparaison Temporelle

**RÃ´le :** Analyse comparative entre deux pÃ©riodes.

| Attribut | Type | Description |
|----------|------|-------------|
| id | UUID | Identifiant unique |
| model_id | FK | RÃ©fÃ©rence au modÃ¨le |
| metric_name | String | MÃ©trique comparÃ©e |
| period_type | Enum | week / month / quarter / year / custom |
| period_start | DateTime | DÃ©but de la pÃ©riode |
| period_end | DateTime | Fin de la pÃ©riode |
| value_start | Decimal | Valeur au dÃ©but |
| value_end | Decimal | Valeur Ã  la fin |
| delta | Decimal | DiffÃ©rence absolue |
| delta_percentage | Decimal | DiffÃ©rence en pourcentage |
| created_at | DateTime | Date de crÃ©ation |

---

## 6. SpÃ©cifications Fonctionnelles par Module

### 6.1 Module Provider Manager

#### Description
GÃ¨re la connexion Ã  OpenRouter et l'abstraction pour futurs providers.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| Connexion Ã  OpenRouter | ğŸ”´ MANDATORY | Provider initial unique |
| Abstraction provider (interface commune) | ğŸ”´ MANDATORY | PrÃ©pare extensibilitÃ© |
| Gestion des credentials sÃ©curisÃ©e | ğŸ”´ MANDATORY | Env vars, secrets manager |
| Gestion des erreurs API | ğŸ”´ MANDATORY | Codes HTTP, timeouts |
| Retry avec backoff exponentiel | ğŸŸ  MVP | 3 tentatives max |
| Rate limiting intelligent | ğŸŸ  MVP | Respect des limites provider |
| Health check des providers | ğŸ”µ NICE-TO-HAVE | VÃ©rification de disponibilitÃ© |
| Support multi-providers | ğŸ”µ NICE-TO-HAVE | OpenAI, Anthropic, etc. |
| Fallback automatique entre providers | âšª FUTURE | Si un provider fail |

#### Interface Attendue

```
ProviderInterface:
    - connect(credentials) â†’ Connection
    - listModels() â†’ Model[]
    - sendPrompt(model, prompt, config) â†’ Response
    - getUsage(response) â†’ UsageMetrics
    - healthCheck() â†’ HealthStatus
```

### 6.2 Module Model Registry

#### Description
Catalogue des modÃ¨les disponibles avec leurs mÃ©tadonnÃ©es et labels.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| CRUD modÃ¨les de base | ğŸ”´ MANDATORY | Ajouter, modifier, supprimer |
| Association modÃ¨le-provider | ğŸ”´ MANDATORY | Chaque modÃ¨le appartient Ã  un provider |
| Labels personnalisÃ©s | ğŸ”´ MANDATORY | **Demande explicite** |
| Statut actif/inactif | ğŸ”´ MANDATORY | Pour filtrer les benchmarks |
| MÃ©tadonnÃ©es de base (contexte, coÃ»t) | ğŸŸ  MVP | Informations clÃ©s |
| Tags/Ã©tiquettes personnalisÃ©es | ğŸ”µ NICE-TO-HAVE | Filtrage avancÃ© |
| Versioning des modÃ¨les | ğŸ”µ NICE-TO-HAVE | Suivre les versions |
| Sync automatique via API OpenRouter | âšª FUTURE | DÃ©tection nouveaux modÃ¨les |
| Comparaison auto nouveau vs ancien | âšª FUTURE | Benchmark automatique des nouveaux |

### 6.3 Module Question Bank

#### Description
Gestion des questions de benchmark organisÃ©es en suites.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| CRUD questions | ğŸ”´ MANDATORY | Base de questions |
| Types de questions | ğŸ”´ MANDATORY | CatÃ©gorisation |
| Association question-type | ğŸ”´ MANDATORY | Chaque question a un type |
| Suites de questions | ğŸŸ  MVP | Regroupement cohÃ©rent |
| Versioning des questions | ğŸŸ  MVP | TraÃ§abilitÃ© des changements |
| Versioning des suites | ğŸŸ  MVP | **Demande explicite** |
| Import/export questions | ğŸ”µ NICE-TO-HAVE | JSON, CSV |
| GÃ©nÃ©rateur de questions via LLM | âšª FUTURE | Questions dynamiques |
| Validation automatique de qualitÃ© | âšª FUTURE | VÃ©rification cohÃ©rence |

#### Questions Statiques vs Dynamiques

| Type | Description | Usage |
|------|-------------|-------|
| **Statiques** | Toujours les mÃªmes questions | Mesurer l'Ã©volution d'un modÃ¨le dans le temps |
| **Dynamiques** | GÃ©nÃ©rÃ©es selon l'actualitÃ© ou le contexte | Tester l'adaptation aux nouvelles informations |

### 6.4 Module Benchmark Executor

#### Description
Orchestration des exÃ©cutions de benchmarks.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| ExÃ©cution manuelle | ğŸ”´ MANDATORY | Pour tests et debug |
| ExÃ©cution complÃ¨te d'une suite | ğŸ”´ MANDATORY | Core functionality |
| Capture des rÃ©ponses | ğŸ”´ MANDATORY | Stockage brut |
| Mesure du temps de rÃ©ponse | ğŸ”´ MANDATORY | MÃ©trique clÃ© |
| Calcul des tokens/coÃ»ts | ğŸŸ  MVP | Suivi budgÃ©taire |
| NumÃ©rotation des itÃ©rations | ğŸŸ  MVP | **Demande explicite** |
| ExÃ©cution planifiÃ©e automatique | ğŸŸ  MVP | Autonomie |
| Gestion des erreurs (retry) | ğŸŸ  MVP | Robustesse |
| ExÃ©cution partielle (subset de modÃ¨les) | ğŸ”µ NICE-TO-HAVE | FlexibilitÃ© |
| ParallÃ©lisation des requÃªtes | ğŸ”µ NICE-TO-HAVE | Performance |
| Reprise aprÃ¨s Ã©chec | ğŸ”µ NICE-TO-HAVE | RÃ©silience |
| Mode dry-run | ğŸ”µ NICE-TO-HAVE | Test sans appels API |

#### Pipeline d'ExÃ©cution (Toujours Identique)

1. Initialisation : VÃ©rification des providers actifs
2. Chargement : Config campagne, suites, modÃ¨les
3. GÃ©nÃ©ration : Liste cartÃ©sienne modÃ¨le Ã— question
4. Injection : PrÃ©paration du prompt avec contexte
5. ExÃ©cution : Envoi des requÃªtes (gestion retries, timeouts)
6. Capture : RÃ©ponse brute, temps, coÃ»t, tokens
7. Persistance : Stockage immÃ©diat des rÃ©sultats

### 6.5 Module Evaluator

#### Description
Ã‰valuation des rÃ©ponses et attribution des scores.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| Ã‰valuation rule-based simple | ğŸ”´ MANDATORY | Exact match, regex |
| LLM-as-Judge basique | ğŸŸ  MVP | Un modÃ¨le juge |
| CritÃ¨res d'Ã©valuation configurables | ğŸŸ  MVP | Par question |
| Score normalisÃ© | ğŸŸ  MVP | Ã‰chelle commune |
| Justification de l'Ã©valuation | ğŸŸ  MVP | Transparence |
| Dimensions multiples de score | ğŸŸ  MVP | Accuracy, style, etc. |
| Multi-juges (consensus) | ğŸ”µ NICE-TO-HAVE | Moyenne ou vote |
| Calibration du juge | ğŸ”µ NICE-TO-HAVE | CohÃ©rence |
| MÃ©triques NLP avancÃ©es | ğŸ”µ NICE-TO-HAVE | BLEU, ROUGE, etc. |
| Human-in-the-loop | âšª FUTURE | Validation humaine |
| Fine-tuning du juge | âšª FUTURE | ModÃ¨le spÃ©cialisÃ© |

#### Types d'Ã‰valuation

| Type | Description | Cas d'Usage |
|------|-------------|-------------|
| **Exact Match** | RÃ©ponse == attendu | Questions factuelles |
| **Regex** | Pattern matching | Formats spÃ©cifiques |
| **Inclusion** | Mots-clÃ©s prÃ©sents | RÃ©ponses ouvertes |
| **LLM Judge** | Autre LLM note la rÃ©ponse | QualitÃ© gÃ©nÃ©rale |
| **Tests Unitaires** | ExÃ©cution de code | Questions de coding |
| **MÃ©triques NLP** | BLEU, ROUGE, etc. | Traduction, rÃ©sumÃ© |

### 6.6 Module Ranking Engine

#### Description
Calcul et gestion des rankings multi-dimensionnels.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| Ranking global simple | ğŸ”´ MANDATORY | Score moyen par modÃ¨le |
| Stockage ranking en BDD dÃ©diÃ©e | ğŸ”´ MANDATORY | **Table propre demandÃ©e** |
| Ranking par type de question | ğŸŸ  MVP | **Demande explicite** |
| Ranking avec dates | ğŸŸ  MVP | **Demande explicite** |
| Ranking comparatif entre LLM | ğŸŸ  MVP | **Demande explicite** |
| Ranking par itÃ©ration | ğŸŸ  MVP | **Demande explicite** |
| Position + delta vs prÃ©cÃ©dent | ğŸŸ  MVP | Ã‰volution |
| PondÃ©ration des critÃ¨res | ğŸ”µ NICE-TO-HAVE | Personnalisation |
| Ranking ELO-style | ğŸ”µ NICE-TO-HAVE | Comparaisons 1v1 |
| Ranking personnalisable | ğŸ”µ NICE-TO-HAVE | Filtres custom |
| PrÃ©diction de ranking | âšª FUTURE | ML sur historique |

#### Dimensions du Ranking

| Dimension | Description |
|-----------|-------------|
| **Temporelle** | Rang Ã  l'instant T, Ã©volution vs T-1 |
| **CatÃ©gorielle** | Rang sur Logique, CrÃ©ativitÃ©, Code, etc. |
| **Comparative** | Rang relatif (Elo) par rapport aux autres modÃ¨les actifs |
| **Contextuelle** | Performance selon l'Ã©tat du monde |

### 6.7 Module Temporal Analyzer

#### Description
Analyses temporelles et comparaisons dans le temps.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| Stockage historique complet | ğŸ”´ MANDATORY | Base de l'analyse |
| Comparaison WoW (Week-over-Week) | ğŸŸ  MVP | **Demande explicite** |
| Comparaison MoM (Month-over-Month) | ğŸŸ  MVP | **Demande explicite** |
| Delta calculÃ© (absolu et %) | ğŸŸ  MVP | Quantification |
| Comparaison QoQ/YoY | ğŸ”µ NICE-TO-HAVE | Long terme |
| Graphique d'Ã©volution | ğŸ”µ NICE-TO-HAVE | Visualisation |
| DÃ©tection de tendances | ğŸ”µ NICE-TO-HAVE | RÃ©gression |
| Alertes sur changements significatifs | âšª FUTURE | Notifications |
| PrÃ©dictions | âšª FUTURE | Projection |

#### MÃ©triques d'Ã‰volution

| MÃ©trique | Calcul | Usage |
|----------|--------|-------|
| Delta absolu | Score(T) - Score(T-1) | Changement brut |
| Delta relatif (%) | ((Score(T) - Score(T-1)) / Score(T-1)) Ã— 100 | Changement proportionnel |
| Position delta | Rang(T) - Rang(T-1) | Mouvement dans le classement |
| Tendance | RÃ©gression linÃ©aire sur N pÃ©riodes | Direction gÃ©nÃ©rale |
| VolatilitÃ© | Ã‰cart-type sur N pÃ©riodes | StabilitÃ© du modÃ¨le |

### 6.8 Module Memory Manager

#### Description
Gestion de la mÃ©moire, des itÃ©rations et de la persistance.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| Persistance en base de donnÃ©es | ğŸ”´ MANDATORY | Core |
| Historique des rÃ©sultats | ğŸ”´ MANDATORY | Tout est gardÃ© |
| Champ iteration_id | ğŸ”´ MANDATORY | Version de l'expÃ©rience |
| Snapshots d'Ã©tat | ğŸŸ  MVP | Photo Ã  un instant T |
| TraÃ§abilitÃ© des changements d'itÃ©ration | ğŸŸ  MVP | Audit |
| Release notes par itÃ©ration | ğŸ”µ NICE-TO-HAVE | Documentation |
| Purge/archivage configurables | ğŸ”µ NICE-TO-HAVE | Gestion espace |
| Backup automatique | ğŸ”µ NICE-TO-HAVE | SÃ©curitÃ© |
| Restauration point-in-time | âšª FUTURE | RÃ©cupÃ©ration |

#### Concept de MÃ©moire

La mÃ©moire du systÃ¨me permet de :
- Rejouer une expÃ©rience prÃ©cÃ©dente exactement
- Comprendre dans quelles conditions un score a Ã©tÃ© obtenu
- Situer un rÃ©sultat dans l'histoire du systÃ¨me
- Comparer des pommes avec des pommes (mÃªme itÃ©ration)

### 6.9 Module Reporter

#### Description
GÃ©nÃ©ration des rapports et exports.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| Log d'exÃ©cution | ğŸ”´ MANDATORY | Debug, audit |
| Rapport textuel basique | ğŸŸ  MVP | RÃ©sultats lisibles |
| Export JSON | ğŸŸ  MVP | IntÃ©gration |
| Tableaux comparatifs | ğŸŸ  MVP | Rankings |
| Export CSV | ğŸ”µ NICE-TO-HAVE | Analyse externe |
| Visualisations graphiques | ğŸ”µ NICE-TO-HAVE | Courbes, heatmaps |
| Dashboard temps rÃ©el | ğŸ”µ NICE-TO-HAVE | Monitoring |
| Export PDF | âšª FUTURE | Rapports formels |
| IntÃ©gration BI externe | âšª FUTURE | Metabase, etc. |

#### Types de Rapports

| Type | Description |
|------|-------------|
| **Rapport d'ExÃ©cution** | RÃ©sultats d'un run unique |
| **Rapport Comparatif** | ModÃ¨le A vs ModÃ¨le B |
| **Rapport d'Ã‰volution** | Un modÃ¨le dans le temps |
| **Rapport par CatÃ©gorie** | Meilleurs modÃ¨les par type de question |

### 6.10 Module Scheduler

#### Description
Planification des exÃ©cutions automatiques.

#### FonctionnalitÃ©s

| FonctionnalitÃ© | PrioritÃ© | Notes |
|----------------|----------|-------|
| ExÃ©cution planifiÃ©e (cron-like) | ğŸŸ  MVP | Autonomie |
| Support de plusieurs campagnes | ğŸŸ  MVP | FlexibilitÃ© |
| Gestion des Ã©checs (log, alerte) | ğŸŸ  MVP | Robustesse |
| DÃ©clenchement manuel | ğŸ”´ MANDATORY | Override |
| Dashboard runs passÃ©s/Ã  venir | ğŸ”µ NICE-TO-HAVE | VisibilitÃ© |
| Pause/reprise d'une campagne | ğŸ”µ NICE-TO-HAVE | ContrÃ´le |
| Conditions de dÃ©clenchement | âšª FUTURE | Si coÃ»t < X, etc. |

---

## 7. Flux et Comportements du SystÃ¨me

### 7.1 Flux Principal : ExÃ©cution d'un Benchmark

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DÃ‰CLENCHEUR                                                             â”‚
â”‚ (Planificateur automatique OU dÃ©clenchement manuel)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1 : INITIALISATION                                                â”‚
â”‚ â”œâ”€ Charger le profil de benchmark                                       â”‚
â”‚ â”œâ”€ VÃ©rifier les providers actifs                                        â”‚
â”‚ â”œâ”€ RÃ©cupÃ©rer la liste des modÃ¨les actifs                                â”‚
â”‚ â”œâ”€ Charger la suite de questions                                        â”‚
â”‚ â”œâ”€ CrÃ©er l'enregistrement d'exÃ©cution (nouvelle itÃ©ration)              â”‚
â”‚ â””â”€ Snapshot de la configuration                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2 : BOUCLE D'EXÃ‰CUTION                                            â”‚
â”‚ Pour chaque MODÃˆLE:                                                     â”‚
â”‚   Pour chaque QUESTION:                                                 â”‚
â”‚     â”œâ”€ PrÃ©parer le prompt (avec contexte si applicable)                 â”‚
â”‚     â”œâ”€ Envoyer au provider via l'abstraction                            â”‚
â”‚     â”œâ”€ Capturer: rÃ©ponse, temps, tokens, coÃ»t                           â”‚
â”‚     â”œâ”€ Persister le rÃ©sultat brut immÃ©diatement                         â”‚
â”‚     â””â”€ GÃ©rer les erreurs (retry si nÃ©cessaire)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3 : Ã‰VALUATION                                                    â”‚
â”‚ Pour chaque RÃ‰SULTAT:                                                   â”‚
â”‚   â”œâ”€ Appliquer l'Ã©valuation automatique si applicable                   â”‚
â”‚   â”œâ”€ Si LLM-as-Judge: envoyer au modÃ¨le Ã©valuateur                      â”‚
â”‚   â”œâ”€ Calculer le score brut                                             â”‚
â”‚   â”œâ”€ Normaliser le score                                                â”‚
â”‚   â””â”€ Persister l'Ã©valuation                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4 : CALCUL DES RANKINGS                                           â”‚
â”‚ â”œâ”€ AgrÃ©ger les scores par modÃ¨le                                        â”‚
â”‚ â”œâ”€ Calculer le ranking global                                           â”‚
â”‚ â”œâ”€ Calculer les rankings par type de question                           â”‚
â”‚ â”œâ”€ Comparer avec l'itÃ©ration prÃ©cÃ©dente (deltas)                        â”‚
â”‚ â””â”€ Persister tous les rankings                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5 : ANALYSE TEMPORELLE                                            â”‚
â”‚ â”œâ”€ Calculer les comparaisons pÃ©riode/pÃ©riode (WoW, MoM)                 â”‚
â”‚ â”œâ”€ Mettre Ã  jour les tendances                                          â”‚
â”‚ â””â”€ DÃ©tecter les changements significatifs                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 6 : FINALISATION                                                  â”‚
â”‚ â”œâ”€ CrÃ©er le snapshot final                                              â”‚
â”‚ â”œâ”€ GÃ©nÃ©rer le rapport d'exÃ©cution                                       â”‚
â”‚ â”œâ”€ Mettre Ã  jour le statut de l'exÃ©cution                               â”‚
â”‚ â””â”€ Notifier si configurÃ©                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Ã‰tats du SystÃ¨me

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                                              â”‚
     â–¼                                                              â”‚
   IDLE â”€â”€â–º INITIALIZING â”€â”€â–º EXECUTING â”€â”€â–º EVALUATING â”€â”€â–º          â”‚
                â”‚                â”‚             â”‚                    â”‚
                â”‚                â”‚             â”‚                    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                                 â”‚                                  â”‚
                              FAILED                                â”‚
                                 â”‚                                  â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        (retry ou abandon)

   EVALUATING â”€â”€â–º RANKING â”€â”€â–º ANALYZING â”€â”€â–º FINALIZING â”€â”€â–º IDLE
```

### 7.3 Gestion des Erreurs

| Type d'Erreur | Comportement |
|---------------|--------------|
| Timeout API | Retry avec backoff exponentiel, max 3 tentatives |
| Rate limit | Pause (respect header Retry-After), puis retry |
| Erreur auth | Stop exÃ©cution, alerte immÃ©diate |
| RÃ©ponse invalide | Logger, marquer comme Ã©chec, continuer |
| Erreur Ã©valuateur | Logger, marquer comme non-Ã©valuÃ©, continuer |
| Erreur systÃ¨me | Stop exÃ©cution, Ã©tat sauvegardÃ© pour reprise |

### 7.4 Garanties de DÃ©terminisme

Le systÃ¨me garantit que pour :
- La mÃªme configuration
- Les mÃªmes modÃ¨les
- Les mÃªmes questions
- Les mÃªmes paramÃ¨tres d'appel

Le **comportement du systÃ¨me** sera identique (seules les rÃ©ponses des LLM varieront).

**Ã‰lÃ©ments garantissant le dÃ©terminisme :**
- Seed alÃ©atoire fixÃ© si applicable
- Ordre d'exÃ©cution dÃ©fini (pas de parallÃ©lisation non-dÃ©terministe dans MVP)
- Logging complet pour reproduction
- Configuration versionnÃ©e et snapshotÃ©e

---

## 8. Dimensions Temporelles et MÃ©moire

### 8.1 GranularitÃ© des DonnÃ©es Temporelles

| Niveau | Description | Usage |
|--------|-------------|-------|
| **RÃ©ponse** | Timestamp exact de chaque rÃ©ponse | Analyse fine, latence |
| **Question** | AgrÃ©gation par question dans un run | Comparaison inter-modÃ¨le |
| **ExÃ©cution/Run** | Une exÃ©cution complÃ¨te | Point de rÃ©fÃ©rence principal |
| **Jour** | AgrÃ©gation journaliÃ¨re | Tendances court terme |
| **Semaine** | AgrÃ©gation hebdomadaire | WoW - **demandÃ© explicitement** |
| **Mois** | AgrÃ©gation mensuelle | MoM - **demandÃ© explicitement** |
| **Trimestre** | AgrÃ©gation trimestrielle | QoQ - tendances moyen terme |
| **AnnÃ©e** | AgrÃ©gation annuelle | YoY - tendances long terme |

### 8.2 Types de Comparaisons

#### Comparaison SÃ©quentielle
- ItÃ©ration N vs ItÃ©ration N-1
- Semaine S vs Semaine S-1
- Mois M vs Mois M-1

#### Comparaison Ã  Date Fixe
- Semaine actuelle vs mÃªme semaine annÃ©e prÃ©cÃ©dente
- Snapshot actuel vs snapshot de rÃ©fÃ©rence

#### Comparaison de Plage
- Moyenne Q1 vs Q4
- Ã‰volution sur les 12 derniers mois

### 8.3 Concept d'ItÃ©ration

Une **itÃ©ration** reprÃ©sente une version stable de l'expÃ©rience de benchmark :
- Version des suites de questions utilisÃ©es
- Version du prompt et de la mÃ©thodologie d'Ã©valuation
- Ã‰tat du catalogue de modÃ¨les Ã  cet instant

**Exemple :**
- `ITER-2025-01` : PremiÃ¨re version, benchmark reasoning v1, 50 questions
- `ITER-2025-02` : Ajout de 10 questions, changement du prompt de judge
- `ITER-2025-03` : Ajout de 5 nouveaux modÃ¨les

**RÃ¨gle clÃ© :** Les rÃ©sultats d'un run sont toujours liÃ©s Ã  leur itÃ©ration. On ne compare que des runs de mÃªme itÃ©ration pour Ã©viter les biais.

### 8.4 RÃ©tention des DonnÃ©es

| Type de DonnÃ©e | RÃ©tention | Notes |
|----------------|-----------|-------|
| RÃ©sultats bruts | IllimitÃ©e | NÃ©cessaire pour recalculs |
| Ã‰valuations | IllimitÃ©e | Audit trail |
| Rankings | IllimitÃ©e | Historique essentiel |
| Snapshots | 1 an dÃ©taillÃ©, puis agrÃ©gÃ© | Optimisation espace |
| Logs | 90 jours | Troubleshooting |

---

## 9. ModÃ¨le de DonnÃ©es Conceptuel

### 9.1 SchÃ©ma Relationnel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROVIDER   â”‚       â”‚    MODEL    â”‚       â”‚  QUESTION   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id          â”‚â”€â”€â”    â”‚ id          â”‚   â”Œâ”€â”€â”€â”‚ id          â”‚
â”‚ name        â”‚  â”‚    â”‚ provider_id â”‚â—„â”€â”€â”˜   â”‚ content     â”‚
â”‚ api_base    â”‚  â”‚    â”‚ label       â”‚       â”‚ type_id     â”‚â—„â”€â”€â”
â”‚ status      â”‚  â”‚    â”‚ status      â”‚       â”‚ difficulty  â”‚   â”‚
â”‚ config      â”‚  â”‚    â”‚ tags        â”‚       â”‚ version     â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚ metadata    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
                 â”‚          â”‚                                 â”‚
                 â”‚          â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                 â”‚          â”‚        â”‚  QUESTION_TYPE  â”‚      â”‚
                 â”‚          â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
                 â”‚          â”‚        â”‚ id              â”‚â”€â”€â”€â”€â”€â”€â”˜
                 â”‚          â”‚        â”‚ name            â”‚
                 â”‚          â”‚        â”‚ weight          â”‚
                 â”‚          â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚          â”‚
                 â”‚          â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚          â”‚        â”‚ QUESTION_SUITE  â”‚
                 â”‚          â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                 â”‚          â”‚        â”‚ id              â”‚
                 â”‚          â”‚        â”‚ name            â”‚
                 â”‚          â”‚        â”‚ version         â”‚
                 â”‚          â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚          â”‚                 â”‚
                 â”‚          â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚          â”‚        â”‚ SUITE_QUESTION  â”‚
                 â”‚          â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                 â”‚          â”‚        â”‚ suite_id        â”‚
                 â”‚          â”‚        â”‚ question_id     â”‚
                 â”‚          â”‚        â”‚ order           â”‚
                 â”‚          â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚          â”‚
                 â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       BENCHMARK_EXECUTION                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                                                          â”‚
â”‚ campaign_id                                                 â”‚
â”‚ iteration_id                                                â”‚
â”‚ iteration_number                                            â”‚
â”‚ started_at / completed_at                                   â”‚
â”‚ status                                                      â”‚
â”‚ config_snapshot                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BENCHMARK_RESULT                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                                                          â”‚
â”‚ execution_id â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
â”‚ model_id â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
â”‚ question_id â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
â”‚ response_content                                            â”‚
â”‚ response_time_ms / tokens / cost                            â”‚
â”‚ status                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        EVALUATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                                                          â”‚
â”‚ result_id â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
â”‚ evaluator_type (rule_based / llm_judge)                     â”‚
â”‚ evaluator_model_id (si LLM)                                 â”‚
â”‚ dimension                                                   â”‚
â”‚ score / normalized_score                                    â”‚
â”‚ justification                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RANKING                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                                                          â”‚
â”‚ execution_id                                                â”‚
â”‚ ranking_type (global / by_type / comparative)               â”‚
â”‚ dimension                                                   â”‚
â”‚ scope (JSON)                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RANKING_ENTRY                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                                                          â”‚
â”‚ ranking_id                                                  â”‚
â”‚ model_id                                                    â”‚
â”‚ position                                                    â”‚
â”‚ score                                                       â”‚
â”‚ previous_position / delta_position                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ITERATION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                                                          â”‚
â”‚ code (ITER-2025-01)                                         â”‚
â”‚ name / description / changelog                              â”‚
â”‚ is_current                                                  â”‚
â”‚ config_snapshot                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TEMPORAL_COMPARISON                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                                                          â”‚
â”‚ model_id                                                    â”‚
â”‚ metric_name                                                 â”‚
â”‚ period_type (week / month / quarter / year)                 â”‚
â”‚ period_start / period_end                                   â”‚
â”‚ value_start / value_end                                     â”‚
â”‚ delta / delta_percentage                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Relations Principales

```
Provider          1 â”€â”€â”€â”€â”€â”€â”€ N    Model
Model             1 â”€â”€â”€â”€â”€â”€â”€ N    Benchmark_Result
Model             1 â”€â”€â”€â”€â”€â”€â”€ N    Ranking_Entry
Model             1 â”€â”€â”€â”€â”€â”€â”€ N    Evaluation (as evaluator)

Question          N â”€â”€â”€â”€â”€â”€â”€ 1    Question_Type
Question          N â”€â”€â”€â”€â”€â”€â”€ M    Question_Suite (via Suite_Question)

Benchmark_Execution   1 â”€â”€â”€ N    Benchmark_Result
Benchmark_Execution   1 â”€â”€â”€ N    Ranking
Benchmark_Execution   1 â”€â”€â”€ N    Snapshot

Benchmark_Result      1 â”€â”€â”€ N    Evaluation

Ranking               1 â”€â”€â”€ N    Ranking_Entry

Campaign              1 â”€â”€â”€ N    Benchmark_Execution
Iteration             1 â”€â”€â”€ N    Benchmark_Execution
```

### 9.3 Matrice de DÃ©pendances

```
                              DÃ©pend de â†’
                    â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
                    â”‚ Prov â”‚ Model â”‚ Quest â”‚ Exec â”‚ Eval â”‚ Rank â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
       â”‚ Provider   â”‚  -   â”‚       â”‚       â”‚      â”‚      â”‚      â”‚
       â”‚ Model      â”‚  â—   â”‚   -   â”‚       â”‚      â”‚      â”‚      â”‚
       â”‚ Question   â”‚      â”‚       â”‚   -   â”‚      â”‚      â”‚      â”‚
       â”‚ Execution  â”‚      â”‚   â—   â”‚   â—   â”‚  -   â”‚      â”‚      â”‚
       â”‚ Evaluation â”‚      â”‚   â—   â”‚   â—   â”‚  â—   â”‚  -   â”‚      â”‚
       â”‚ Ranking    â”‚      â”‚   â—   â”‚   â—   â”‚  â—   â”‚  â—   â”‚  -   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

       â— = dÃ©pendance directe
```

---

## 10. Priorisation MoSCoW

### 10.1 LÃ©gende

| Code | CatÃ©gorie | DÃ©finition |
|------|-----------|------------|
| ğŸ”´ | MANDATORY | Indispensable au fonctionnement de base. Le systÃ¨me ne peut pas exister sans. |
| ğŸŸ  | MVP | NÃ©cessaire pour une premiÃ¨re version utilisable et dÃ©montrable. |
| ğŸ”µ | NICE-TO-HAVE | AmÃ©lioration significative, non bloquante pour le lancement. |
| âšª | FUTURE | Vision long terme, hors scope initial. |

### 10.2 Vue SynthÃ©tique par Module

#### Provider & ModÃ¨les

| FonctionnalitÃ© | PrioritÃ© |
|----------------|----------|
| Connexion OpenRouter | ğŸ”´ |
| Abstraction provider | ğŸ”´ |
| Credentials sÃ©curisÃ©s | ğŸ”´ |
| CRUD modÃ¨les | ğŸ”´ |
| Labels personnalisÃ©s | ğŸ”´ |
| Statut actif/inactif | ğŸ”´ |
| Retry avec backoff | ğŸŸ  |
| Rate limiting | ğŸŸ  |
| Tags personnalisÃ©s | ğŸ”µ |
| Multi-providers | ğŸ”µ |
| Sync auto API | âšª |

#### Questions & Suites

| FonctionnalitÃ© | PrioritÃ© |
|----------------|----------|
| CRUD questions | ğŸ”´ |
| Types de questions | ğŸ”´ |
| Suites de questions | ğŸŸ  |
| Versioning | ğŸŸ  |
| Import/export | ğŸ”µ |
| GÃ©nÃ©rateur LLM | âšª |

#### ExÃ©cution

| FonctionnalitÃ© | PrioritÃ© |
|----------------|----------|
| ExÃ©cution manuelle | ğŸ”´ |
| Capture rÃ©ponses | ğŸ”´ |
| Mesure temps | ğŸ”´ |
| Calcul tokens/coÃ»ts | ğŸŸ  |
| NumÃ©ro d'itÃ©ration | ğŸŸ  |
| ExÃ©cution planifiÃ©e | ğŸŸ  |
| ParallÃ©lisation | ğŸ”µ |
| Reprise aprÃ¨s Ã©chec | ğŸ”µ |

#### Ã‰valuation

| FonctionnalitÃ© | PrioritÃ© |
|----------------|----------|
| Rule-based simple | ğŸ”´ |
| LLM-as-Judge | ğŸŸ  |
| Score normalisÃ© | ğŸŸ  |
| Dimensions multiples | ğŸŸ  |
| Multi-juges | ğŸ”µ |
| MÃ©triques NLP | ğŸ”µ |

#### Ranking

| FonctionnalitÃ© | PrioritÃ© |
|----------------|----------|
| Ranking global | ğŸ”´ |
| Table dÃ©diÃ©e BDD | ğŸ”´ |
| Par type de question | ğŸŸ  |
| Avec dates | ğŸŸ  |
| Comparatif entre LLM | ğŸŸ  |
| Par itÃ©ration | ğŸŸ  |
| Position + delta | ğŸŸ  |
| PondÃ©ration | ğŸ”µ |
| ELO-style | ğŸ”µ |

#### Temporel & MÃ©moire

| FonctionnalitÃ© | PrioritÃ© |
|----------------|----------|
| Historique complet | ğŸ”´ |
| Iteration_id | ğŸ”´ |
| Comparaison WoW | ğŸŸ  |
| Comparaison MoM | ğŸŸ  |
| Snapshots | ğŸŸ  |
| Graphiques Ã©volution | ğŸ”µ |
| Alertes changements | âšª |

#### Reporting

| FonctionnalitÃ© | PrioritÃ© |
|----------------|----------|
| Logs d'exÃ©cution | ğŸ”´ |
| Rapport textuel | ğŸŸ  |
| Export JSON | ğŸŸ  |
| Tableaux comparatifs | ğŸŸ  |
| Export CSV | ğŸ”µ |
| Dashboard | ğŸ”µ |
| Export PDF | âšª |

---

## 11. Roadmap par Phases

### Phase 0 : Concept & Documentation
**Objectif :** Finaliser la conception avant tout dÃ©veloppement.

| Livrable | Description |
|----------|-------------|
| âœ… Ce document | Bible du projet |
| DÃ©finition des premiÃ¨res suites | 20-50 questions de test |
| Choix technologiques | Stack, BDD, hÃ©bergement |
| RÃ©ponses aux questions ouvertes | DÃ©cisions clÃ©s |

### Phase 1 : Squelette / Fondations
**Objectif :** Le systÃ¨me existe et peut faire un run basique.

| Livrable | Description |
|----------|-------------|
| Connecteur OpenRouter | Connexion API stable |
| Registre des modÃ¨les | CRUD basique |
| Boucle d'exÃ©cution simple | Questions fixes â†’ ModÃ¨les fixes |
| Stockage brut | Sauvegarde rÃ©ponses + timestamp |
| Structure de ranking V1 | Table simple : ModÃ¨le_ID \| Date \| Score |

**CritÃ¨re de succÃ¨s :** Pouvoir lancer manuellement un benchmark sur 3 modÃ¨les, 10 questions, et voir un ranking basique.

### Phase 2 : MVP Autonome
**Objectif :** Le systÃ¨me tourne tout seul et produit des rÃ©sultats exploitables.

| Livrable | Description |
|----------|-------------|
| ExÃ©cution planifiÃ©e | Job hebdomadaire automatique |
| LLM-as-Judge basique | Un modÃ¨le juge les rÃ©ponses |
| Rankings multi-dimensions | Par type de question |
| Labels dynamiques | Tags sur les modÃ¨les |
| Comparaison WoW | Semaine N vs N-1 |

**CritÃ¨re de succÃ¨s :** Le systÃ¨me fait un benchmark hebdo automatique, produit des rankings par catÃ©gorie, et montre l'Ã©volution d'une semaine Ã  l'autre.

### Phase 3 : MÃ©moire & Profondeur
**Objectif :** Le systÃ¨me a une vraie mÃ©moire et permet des analyses riches.

| Livrable | Description |
|----------|-------------|
| ItÃ©rations structurÃ©es | Changelog, versioning |
| Comparaison MoM | Mois par mois |
| Snapshots complets | Photo de l'Ã©tat Ã  chaque run |
| Dimensions multiples de score | Accuracy, style, reasoning |
| Reporting avancÃ© | Export JSON/CSV, tableaux |

**CritÃ¨re de succÃ¨s :** Pouvoir comparer les performances d'un modÃ¨le sur 3 mois avec drill-down par dimension.

### Phase 4 : ExtensibilitÃ© & UI
**Objectif :** Le systÃ¨me est mature et utilisable par d'autres.

| Livrable | Description |
|----------|-------------|
| Dashboard web | Interface de consultation |
| Visualisations graphiques | Courbes d'Ã©volution |
| Alertes | Notifications sur changements significatifs |
| Multi-providers (prÃ©paration) | Architecture prÃªte |
| Documentation utilisateur | Guide d'utilisation |

### Phase 5 : Vision Lointaine
| Livrable | Description |
|----------|-------------|
| Multi-providers actifs | OpenAI direct, Anthropic, etc. |
| Benchmark adversarial | Deux modÃ¨les dÃ©battent |
| Fine-tuning du juge | ModÃ¨le spÃ©cialisÃ© |
| Injection de World State | Questions contextuelles auto |
| Auto-sÃ©lection de modÃ¨le | Choix optimal par use case |

---

## 12. ConsidÃ©rations Transversales

### 12.1 SÃ©curitÃ©

| Aspect | Mesure |
|--------|--------|
| Credentials | Stockage sÃ©curisÃ© (env vars, secrets manager). Jamais en dur dans le code. |
| API Keys | Ne jamais logger, ne jamais exposer dans les rapports |
| DonnÃ©es | Chiffrement au repos si donnÃ©es sensibles dans les questions |
| AccÃ¨s | Authentification pour interface admin (si applicable) |

### 12.2 Performance

| Aspect | Mesure |
|--------|--------|
| RequÃªtes API | Respect strict des rate limits |
| Base de donnÃ©es | Indexation appropriÃ©e (model_id, execution_id, date) |
| Historique | Partitionnement si volume > 1M enregistrements |
| MÃ©moire | Streaming des rÃ©ponses longues |

### 12.3 ObservabilitÃ©

| Aspect | Mesure |
|--------|--------|
| Logging | StructurÃ© (JSON), niveaux appropriÃ©s (DEBUG/INFO/ERROR) |
| MÃ©triques | Temps d'exÃ©cution, taux d'erreur, coÃ»ts par run |
| Tracing | CorrÃ©lation des requÃªtes (trace_id) |
| Alerting | Ã‰checs de run, anomalies de performance |

### 12.4 MaintenabilitÃ©

| Aspect | Mesure |
|--------|--------|
| Code | SÃ©paration claire des responsabilitÃ©s (modules) |
| Configuration | ExternalisÃ©e, versionnÃ©e, documentÃ©e |
| Documentation | Ã€ jour avec le code (README par module) |
| Tests | Couverture des flux critiques (exÃ©cution, Ã©valuation, ranking) |

### 12.5 CoÃ»ts

| Poste | ConsidÃ©ration |
|-------|---------------|
| Appels LLM (benchmarks) | Budget Ã  dÃ©finir par run |
| Appels LLM (juge) | Potentiellement le plus gros poste |
| Stockage | Croissance linÃ©aire avec historique |
| Compute | GÃ©nÃ©ralement faible (orchestration) |

**Estimation prÃ©liminaire :**
- 50 questions Ã— 20 modÃ¨les Ã— 1000 tokens moyen = 1M tokens/run
- Si juge = 500 tokens/Ã©valuation Ã— 1000 Ã©valuations = 500k tokens/run
- Total : ~1.5M tokens/run

---

## 13. Questions Ouvertes et DÃ©cisions Ã  Prendre

### 13.1 Questions Techniques

| # | Question | Options | Impact |
|---|----------|---------|--------|
| T1 | Quelle base de donnÃ©es ? | PostgreSQL, MySQL, MongoDB | Architecture, performances |
| T2 | Quel framework/langage ? | PHP/Symfony, Python/FastAPI, Node/NestJS | Stack, maintenance |
| T3 | HÃ©bergement ? | Local, Cloud (AWS/GCP/Azure), Docker | CoÃ»ts, scalabilitÃ© |
| T4 | Scheduler ? | Cron systÃ¨me, Symfony Messenger, Celery | Robustesse |

### 13.2 Questions Fonctionnelles

| # | Question | Options | Impact |
|---|----------|---------|--------|
| F1 | FrÃ©quence de benchmark cible ? | Quotidien, Hebdo, Mensuel | CoÃ»ts, pertinence |
| F2 | Volume de modÃ¨les initial ? | 10, 20, 50+ | CoÃ»ts, temps d'exÃ©cution |
| F3 | Volume de questions par suite ? | 20, 50, 100+ | Profondeur vs coÃ»t |
| F4 | Quel modÃ¨le comme juge initial ? | GPT-4, Claude 3 Opus, autre | QualitÃ©, coÃ»t |
| F5 | Types de questions prioritaires ? | Reasoning, Code, General | Focus initial |
| F6 | Besoin d'UI V1 ? | Oui/Non | Scope, timeline |

### 13.3 Questions Business

| # | Question | Options | Impact |
|---|----------|---------|--------|
| B1 | Budget API mensuel estimÃ© ? | $50, $200, $500+ | Contraintes scope |
| B2 | Timeline souhaitÃ©e pour MVP ? | 1 mois, 3 mois, 6 mois | Planning |
| B3 | Usage interne ou externe ? | Interne ERA, Produit | Exigences qualitÃ© |
| B4 | DonnÃ©es sensibles dans questions ? | Oui/Non | SÃ©curitÃ© requise |

### 13.4 DÃ©cisions Ã  Documenter

Chaque dÃ©cision prise devra Ãªtre documentÃ©e avec :
- **Date** de la dÃ©cision
- **Contexte** de la dÃ©cision
- **Options** considÃ©rÃ©es
- **Choix** final
- **Raison** du choix

---

## 14. Annexes

### Annexe A : RÃ©capitulatif des Demandes Explicites

Les Ã©lÃ©ments suivants ont Ã©tÃ© **explicitement demandÃ©s** et doivent Ãªtre prÃ©sents :

1. âœ… SystÃ¨me autonome de benchmarking
2. âœ… Peu d'interactions / comportement dÃ©terministe
3. âœ… Comparaisons temporelles (semaine/semaine, mois/mois, etc.)
4. âœ… Connexion OpenRouter en premier
5. âœ… Lecture de documentation API
6. âœ… Ajout facile de nouveaux modÃ¨les
7. âœ… Labels personnalisÃ©s sur les modÃ¨les
8. âœ… Rankings avec leur propre table (entitÃ© sÃ©parÃ©e)
9. âœ… Rankings par type de question
10. âœ… Rankings avec dates
11. âœ… Rankings avec comparaisons entre LLM
12. âœ… Rankings par itÃ©ration
13. âœ… Gestion de la mÃ©moire/historique
14. âœ… Approche step-by-step / dÃ©coupage en phases
15. âœ… Chaque concept isolÃ©

### Annexe B : Personas et Usages

#### AI Director / Lead (Tom)
- **Besoin :** Vision macro des tendances, choix stratÃ©gique de modÃ¨les
- **Usage :** Consulte dashboards, snapshots de rankings, diffs semaine/mois

#### Tech Lead / Dev
- **Besoin :** Savoir quels modÃ¨les privilÃ©gier par use case
- **Usage :** Consulte classements par type de tÃ¢che (code, reasoning)

#### Data / AI Engineer
- **Besoin :** VÃ©rifier si un changement de dataset modifie les rÃ©sultats
- **Usage :** Utilise la mÃ©moire (itÃ©rations) pour analyser les impacts

#### Stakeholders Non Techniques
- **Besoin :** Confiance dans les choix de modÃ¨les
- **Usage :** Lecture de rapports simplifiÃ©s

### Annexe C : Exemples de Questions de Benchmark

#### Type : Raisonnement Logique
```
Prompt: "Si tous les chats sont des mammifÃ¨res, et que certains mammifÃ¨res
vivent dans l'eau, est-il possible qu'un chat vive dans l'eau ?
Explique ton raisonnement."

Ã‰valuation: LLM-as-Judge sur la qualitÃ© du raisonnement
```

#### Type : Code
```
Prompt: "Ã‰cris une fonction Python qui inverse une liste sans utiliser
la mÃ©thode reverse() ni le slicing [::-1]."

Ã‰valuation: Test unitaire + LLM-as-Judge sur la qualitÃ© du code
```

#### Type : FactualitÃ©
```
Prompt: "Quelle est la capitale de l'Australie ?"

RÃ©ponse attendue: "Canberra"
Ã‰valuation: Exact match (case-insensitive)
```

#### Type : CrÃ©ativitÃ©
```
Prompt: "Ã‰cris un haÃ¯ku sur le passage du temps."

Ã‰valuation: LLM-as-Judge sur crÃ©ativitÃ© et respect du format
```

### Annexe D : Structure de Documentation RecommandÃ©e

```
docs/
â”œâ”€â”€ 00_VISION_GLOBALE.md           â† Contexte, vision, objectifs
â”œâ”€â”€ 01_GLOSSAIRE_CONCEPTS.md       â† Tous les termes dÃ©finis
â”œâ”€â”€ 02_PERSONAS_USAGES.md          â† Acteurs et leurs besoins
â”œâ”€â”€ 03_SPEC_FONCTIONNELLE.md       â† Modules dÃ©taillÃ©s
â”œâ”€â”€ 04_MODELE_DONNEES.md           â† Schema BDD
â”œâ”€â”€ 05_FLUX_COMPORTEMENTS.md       â† Pipelines et Ã©tats
â”œâ”€â”€ 06_PRIORISATION_MOSCOW.md      â† Table de priorisation
â”œâ”€â”€ 07_ROADMAP_PHASES.md           â† Planning par phase
â”œâ”€â”€ 08_DECISIONS_ADR/              â† Architecture Decision Records
â”‚   â”œâ”€â”€ ADR-001-choix-bdd.md
â”‚   â”œâ”€â”€ ADR-002-choix-framework.md
â”‚   â””â”€â”€ ...
â””â”€â”€ 09_QUESTIONS_OUVERTES.md       â† Tracking des dÃ©cisions
```

---

## Signatures et Validation

| RÃ´le | Nom | Date | Signature |
|------|-----|------|-----------|
| Product Owner | Tom | ___ | ___ |
| Tech Lead | ___ | ___ | ___ |
| Reviewer | ___ | ___ | ___ |

---

*Document gÃ©nÃ©rÃ© le 24 novembre 2025*
*Version 1.0 - Document Fondateur*
*Prochaine rÃ©vision prÃ©vue : AprÃ¨s validation des questions ouvertes*

---

> **Ce document est la bible du projet SABE (SystÃ¨me Autonome de Benchmarking Ã‰volutif). Toute modification significative doit Ãªtre versionnÃ©e et validÃ©e.**